{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import eval_func\n",
    "import pandas as pd\n",
    "import json\n",
    "from toolset import Toolset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(eval_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1\n",
    "task1_res_fn = \"../reproduce/output/run_results/task1_results.csv\"\n",
    "task1_out_fn = \"../reproduce/output/run_results/task1_results_eval.csv\"\n",
    "task1_res_df = pd.read_csv(task1_res_fn, index_col=0)\n",
    "task1_conv_list = task1_res_df[\"resulting_conv\"].tolist()\n",
    "task1_conv_list = [json.loads(text) for text in task1_conv_list]\n",
    "label_list = task1_res_df[\"tool_use_label\"].tolist()\n",
    "prec, recall, f1, judgement_list = eval_func.evaluate_task1(task1_conv_list, label_list)\n",
    "print(prec, recall, f1)\n",
    "eval_scores = [score for score, _ in judgement_list]\n",
    "eval_reasons = [reason for _, reason in judgement_list]\n",
    "task1_res_df[\"eval_scores\"] = eval_scores\n",
    "task1_res_df[\"eval_reasons\"] = eval_reasons\n",
    "task1_res_df.to_csv(task1_out_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2\n",
    "task2_res_fn = \"../reproduce/output/run_results/task2_results.csv\"\n",
    "task2_out_fn = \"../reproduce/output/run_results/task2_results_eval.csv\"\n",
    "task2_res_df = pd.read_csv(task2_res_fn, index_col=0)\n",
    "task2_conv_list = task2_res_df[\"resulting_conv\"].tolist()\n",
    "task2_conv_list = [json.loads(text) for text in task2_conv_list]\n",
    "gt_api_name_list = task2_res_df[\"api_name\"].tolist()\n",
    "simi_apis_text_list = task2_res_df[\"simi_apis\"].tolist()\n",
    "simi_apis_list = []\n",
    "for text in simi_apis_text_list:\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    simi_apis = [line.split(\"##\")[1] for line in lines]\n",
    "    simi_apis_list.append(simi_apis)\n",
    "score, precision, percent_of_using_tools, judgement_list = eval_func.evaluate_task2(task2_conv_list, gt_api_name_list, simi_apis_list)\n",
    "print(score, precision, percent_of_using_tools)\n",
    "eval_scores = [score for score, _ in judgement_list]\n",
    "eval_reasons = [reason for _, reason in judgement_list]\n",
    "task2_res_df[\"eval_scores\"] = eval_scores\n",
    "task2_res_df[\"eval_reasons\"] = eval_reasons\n",
    "task2_res_df.to_csv(task2_out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 3\n",
    "task3_res_fn = \"../reproduce/output/run_results/task3_results.csv\"\n",
    "task3_out_fn = \"../reproduce/output/run_results/task3_results_eval.csv\"\n",
    "task3_res_df = pd.read_csv(task3_res_fn, index_col=0)\n",
    "task3_conv_list = task3_res_df[\"resulting_conv\"].tolist()\n",
    "task3_conv_list = [json.loads(text) for text in task3_conv_list]\n",
    "score, judgement_list = eval_func.evaluate_task3(task3_conv_list)\n",
    "print(score)\n",
    "eval_scores = [score for score, _ in judgement_list]\n",
    "eval_reasons = [reason for _, reason in judgement_list]\n",
    "task3_res_df[\"eval_scores\"] = eval_scores\n",
    "task3_res_df[\"eval_reasons\"] = eval_reasons\n",
    "task3_res_df.to_csv(task3_out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4\n",
    "\n",
    "task4_res_fn = \"../reproduce/output/run_results/task4_results.csv\"\n",
    "task4_out_fn = \"../reproduce/output/run_results/task4_results_eval.csv\"\n",
    "\n",
    "task4_res_df = pd.read_csv(task4_res_fn, index_col=0)\n",
    "task4_conv_text_list = task4_res_df[\"resulting_conv\"].tolist()\n",
    "\n",
    "toolset = Toolset()\n",
    "api_docs = toolset.get_api_docs()\n",
    "\n",
    "task4_conv_list = []\n",
    "api_doc_list = []\n",
    "except_idx_list = []\n",
    "normal_idx_list = []\n",
    "except_judgement_list = []\n",
    "for idx in task4_res_df.index:\n",
    "    row = task4_res_df.loc[idx]\n",
    "    tool, api, text = row[[\"tool_name\", \"api_name\", \"resulting_conv\"]].tolist()\n",
    "    try:\n",
    "        conv = json.loads(text)\n",
    "        task4_conv_list.append(conv)\n",
    "        api_doc_list.append(api_docs[tool][api])\n",
    "        normal_idx_list.append(idx)\n",
    "    except Exception as e:\n",
    "        print(e, text)\n",
    "        except_idx_list.append(idx)\n",
    "        except_judgement_list.append((0, \"not a parsable conversation\"))\n",
    "\n",
    "score, precision, judgement_list = eval_func.evaluate_task4(task4_conv_list, api_doc_list)\n",
    "print(score, precision)\n",
    "eval_scores = [score for score, _ in except_judgement_list] +  [score for score, _ in judgement_list]\n",
    "eval_reasons = [reason for _, reason in except_judgement_list] +  [reason for _, reason in judgement_list]\n",
    "out_df = task4_res_df.loc[except_idx_list+normal_idx_list]\n",
    "out_df[\"eval_scores\"] = eval_scores\n",
    "out_df[\"eval_reasons\"] = eval_reasons\n",
    "out_df.to_csv(task4_out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 5\n",
    "task5_res_fn = \"../reproduce/output/run_results/task5_results.csv\"\n",
    "task5_out_fn = \"../reproduce/output/run_results/task5_results_eval.csv\"\n",
    "task5_res_df = pd.read_csv(task5_res_fn, index_col=0)\n",
    "task5_conv_list = task5_res_df[\"resulting_conv\"].tolist()\n",
    "task5_conv_list = [json.loads(text) for text in task5_conv_list]\n",
    "score, judgement_list = eval_func.evaluate_task5(task5_conv_list)\n",
    "print(score)\n",
    "eval_scores = [score for score, _ in judgement_list]\n",
    "eval_reasons = [reason for _, reason in judgement_list]\n",
    "task5_res_df[\"eval_scores\"] = eval_scores\n",
    "task5_res_df[\"eval_reasons\"] = eval_reasons\n",
    "task5_res_df.to_csv(task5_out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 6\n",
    "import pickle\n",
    "task6_res_fn = \"../reproduce/output/run_results/task6_results.csv\"\n",
    "task6_out_fn = \"../reproduce/output/run_results/task6_results_eval.csv\"\n",
    "task6_res_df = pd.read_csv(task6_res_fn)\n",
    "# task6_conv_list = [] \n",
    "task6_conv_list = task6_res_df[\"resulting_conv\"].tolist()\n",
    "task6_conv_list = [json.loads(text) for text in task6_conv_list]\n",
    "score, judgement_list = eval_func.evaluate_task6(task6_conv_list)\n",
    "print(score)\n",
    "\n",
    "eval_scores = [score for score, _ in judgement_list]\n",
    "eval_reasons = [reason for _, reason in judgement_list]\n",
    "task6_res_df[\"eval_scores\"] = eval_scores\n",
    "task6_res_df[\"eval_reasons\"] = eval_reasons\n",
    "task6_res_df.to_csv(task6_out_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 7\n",
    "\n",
    "fn = \"../reproduce/output/run_results/task7_results.csv\"\n",
    "out_fn = \"../reproduce/output/run_results/task7_results_eval.csv\"\n",
    "task7_res_df = pd.read_csv(fn, index_col=0)\n",
    "\n",
    "ref_steps_list = []\n",
    "task7_conv_list = []\n",
    "except_idx_list = []\n",
    "normal_idx_list = []\n",
    "except_judgement_list = []\n",
    "for idx in task7_res_df.index:\n",
    "    row = task7_res_df.loc[idx]\n",
    "    ref_steps_text, conv_text = row[[\"ref_steps\", \"resulting_conv\"]].tolist()\n",
    "    try:\n",
    "        conv = json.loads(conv_text)\n",
    "        task7_conv_list.append(conv)\n",
    "        ref_steps_list.append(ref_steps_text.split(\"\\n\"))\n",
    "        normal_idx_list.append(idx)\n",
    "    except Exception as e:\n",
    "        print(e, idx, conv_text)\n",
    "        except_idx_list.append(idx)\n",
    "        except_judgement_list.append((0, \"not a parsable conversation\"))\n",
    "\n",
    "ave_score, judgement_list = eval_func.evaluate_task7(task7_conv_list, ref_steps_list)\n",
    "print(ave_score)\n",
    "eval_scores = [score for score, _ in except_judgement_list] +  [score for score, _ in judgement_list]\n",
    "eval_reasons = [reason for _, reason in except_judgement_list] +  [reason for _, reason in judgement_list]\n",
    "\n",
    "out_df = task7_res_df.loc[except_idx_list+normal_idx_list]  # [\"eval_scores\"] = eval_scores\n",
    "out_df[\"eval_scores\"] = eval_scores\n",
    "out_df[\"eval_reasons\"] = eval_reasons\n",
    "out_df = out_df.loc[task7_res_df.index]\n",
    "out_df.to_csv(out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 8\n",
    "task8_res_fn = \"../reproduce/output/run_results/task8_results.csv\"\n",
    "task8_out_fn = \"../reproduce/output/run_results/task8_results_eval.csv\"\n",
    "task8_res_df = pd.read_csv(task8_res_fn, index_col=0)\n",
    "task8_conv_list = task8_res_df[\"resulting_conv\"].tolist()\n",
    "task8_conv_list = [json.loads(text) for text in task8_conv_list]\n",
    "score, judgement_list = eval_func.evaluate_task8(task8_conv_list)\n",
    "print(score)\n",
    "eval_scores = [score for score, _ in judgement_list]\n",
    "eval_reasons = [reason for _, reason in judgement_list]\n",
    "task8_res_df[\"eval_scores\"] = eval_scores\n",
    "task8_res_df[\"eval_reasons\"] = eval_reasons\n",
    "task8_res_df.to_csv(task8_out_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e16d08a1e9f924561400ab1dbae52f8e3a6a31fe512ade22523545284255347"
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 ('agent')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
